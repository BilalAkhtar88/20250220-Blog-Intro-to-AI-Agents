{"cells":[{"cell_type":"markdown","metadata":{"id":"6nDaAfItJSL9"},"source":["# Installing and Running Litellm\n","---\n","\n","This notebook demonstrates how to orchestrate multiple LLMs using LiteLLM and Groq, showcasing the potential and challenges of integrating multiple language models.\n","\n","**Limitations Demonstrated**\n","\n","\n","1.   **Error Propagation**: If one model fails (e.g., due to API issues), the orchestrator can still complete using other models but displays error messages for the failed one.\n","2.   **Context Management**: Models operate in isolation without shared context, which may lead to inconsistent answers.\n","3.   **Concurrency and Latency**: Running multiple models concurrently can impact response time depending on API latency.\n","4.   **Scalability Challenges**: Adding more models increases complexity in handling dependencies and error management."]},{"cell_type":"code","source":["!pip install litellm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kiaPn5ttsu5r","executionInfo":{"status":"ok","timestamp":1739722224387,"user_tz":-300,"elapsed":13501,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}},"outputId":"7fdeaebf-714e-48af-b685-485e0cc85a2a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting litellm\n","  Downloading litellm-1.61.6-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm) (3.11.12)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm) (8.1.8)\n","Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (0.28.1)\n","Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (8.6.1)\n","Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm) (3.1.5)\n","Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (4.23.0)\n","Requirement already satisfied: openai>=1.61.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (1.61.1)\n","Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (2.10.6)\n","Collecting python-dotenv>=0.2.0 (from litellm)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Collecting tiktoken>=0.7.0 (from litellm)\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm) (0.21.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.14.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.21.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (25.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.22.3)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.61.0->litellm) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.61.0->litellm) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.61.0->litellm) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.61.0->litellm) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>=1.61.0->litellm) (4.12.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.27.2)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm) (2.32.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (1.3.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (1.18.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm) (0.28.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.17.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.3.0)\n","Downloading litellm-1.61.6-py3-none-any.whl (6.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-dotenv, tiktoken, litellm\n","Successfully installed litellm-1.61.6 python-dotenv-1.0.1 tiktoken-0.9.0\n"]}]},{"cell_type":"code","source":["from litellm import completion\n","from google.colab import userdata\n","import asyncio"],"metadata":{"id":"gKJIr8k1dfGU","executionInfo":{"status":"ok","timestamp":1739722237174,"user_tz":-300,"elapsed":12791,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Retrieve API key securely from Colab's user data storage\n","GROQ_API_KEY = userdata.get('GROQ_API_KEY')"],"metadata":{"id":"rqw2Jk_9dg6I","executionInfo":{"status":"ok","timestamp":1739722238748,"user_tz":-300,"elapsed":1577,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["async def get_groq_model1_response(prompt):\n","    try:\n","        response = completion(\n","            model=\"groq/llama3-8b-8192\",\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            api_key=GROQ_API_KEY\n","        )\n","        return response.choices[0].message.content\n","    except Exception as e:\n","        return f\"Error with Groq Model 1: {e}\""],"metadata":{"id":"w_u7hih4dtEa","executionInfo":{"status":"ok","timestamp":1739722238749,"user_tz":-300,"elapsed":5,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["async def get_groq_model2_response(prompt):\n","    try:\n","        response = completion(\n","            #For demonstration purpose we are using wrong model name in this function leading to error.\n","            model=\"groq/gemma2-model\",\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            api_key=GROQ_API_KEY\n","        )\n","        return response.choices[0].message.content\n","    except Exception as e:\n","        return f\"Error with Groq Model 2: {e}\""],"metadata":{"id":"B9U50iEwdumf","executionInfo":{"status":"ok","timestamp":1739722238749,"user_tz":-300,"elapsed":5,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["async def get_groq_model3_response(prompt):\n","    try:\n","        response = completion(\n","            model=\"groq/llama-3.2-1b-preview\",\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            api_key=GROQ_API_KEY\n","        )\n","        return response.choices[0].message.content\n","    except Exception as e:\n","        return f\"Error with Groq Model 3: {e}\""],"metadata":{"id":"WngpVkJndxtz","executionInfo":{"status":"ok","timestamp":1739722238749,"user_tz":-300,"elapsed":5,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["async def orchestrate_llms(prompt):\n","    print(\"Orchestrating multiple LLMs with LiteLLM...\\n\")\n","\n","    # Wait for all tasks to complete\n","    model1_result = await get_groq_model1_response(prompt + \" and my name is Bilal.\")\n","    model2_result = await get_groq_model2_response(\"Give Concise Summary of\" + model1_result + \"Address the response to user mentioned in previous model.\")\n","    model3_result = await get_groq_model3_response(\"Give detailed explanation of the summary: \" + model2_result + \"Address the response to user mentioned in previous model.\")\n","\n","    # # Run all requests concurrently\n","    # model1_task = asyncio.create_task(get_groq_model1_response(prompt))\n","    # model2_task = asyncio.create_task(get_groq_model2_response(prompt))\n","    # model3_task = asyncio.create_task(get_groq_model3_response(prompt))\n","\n","    # # Wait for all tasks to complete\n","    # model1_result = await model1_task\n","    # model2_result = await model2_task\n","    # model3_result = await model3_task\n","\n","    # Displaying the results\n","    print(\"\\n[Groq Model 1] Philosophical Response:\")\n","    print(model1_result)\n","    print(\"\\n[Groq Model 2] Concise Summary:\")\n","    print(model2_result)\n","    print(\"\\n[Groq Model 3] Detailed Explanation:\")\n","    print(model3_result)"],"metadata":{"id":"KjNbteQpd2hG","executionInfo":{"status":"ok","timestamp":1739722262510,"user_tz":-300,"elapsed":372,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Define the user query\n","prompt = \"What is the meaning of life?\"\n","\n","# Run the orchestration\n","await orchestrate_llms(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-HmHn8hWd5j8","executionInfo":{"status":"ok","timestamp":1739722267039,"user_tz":-300,"elapsed":1289,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}},"outputId":"5b67d925-f575-4f37-ab03-89a14be75329"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Orchestrating multiple LLMs with LiteLLM...\n","\n","\n","\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n","LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n","\n","\n","\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n","\n","\n","[Groq Model 1] Philosophical Response:\n","Hello Bilal!\n","\n","The question \"What is the meaning of life?\" is a profound and age-old inquiry that has puzzled philosophers, theologians, and scientists for centuries. There is no straightforward answer, and the concept of the meaning of life is subjective and varies from person to person.\n","\n","For some, the meaning of life is spiritual, believing that life has a higher purpose or significance assigned by a supreme being or higher power. Others believe that the meaning of life is found through personal fulfillment, happiness, and self-actualization.\n","\n","Some possible answers to the meaning of life might be:\n","\n","1. **Living in the present**: Embracing the beauty and fragility of life, and finding meaning in the here and now.\n","2. **Pursuing happiness**: Seeking joy, contentment, and fulfillment through personal relationships, experiences, and achievements.\n","3. **Making a positive impact**: Leaving a lasting legacy, contributing to the greater good, and making a difference in the lives of others.\n","4. **Exploring and learning**: Continuously seeking knowledge, understanding the mysteries of the universe, and expanding human understanding.\n","5. **Finding purpose**: Identifying one's passions, values, and strengths, and using them to create a sense of direction and fulfillment.\n","\n","As for your name, Bilal, it has a rich history and significance. Bilal is a popular name in many cultures, particularly in the Middle East and North Africa. In Islamic tradition, Bilal ibn Rabah is considered the first muezzin (prayer caller) and is revered as a righteous and trustworthy person. The name Bilal is often associated with virtues such as faithfulness, loyalty, and charity.\n","\n","So, Bilal, what do you think the meaning of life is? How do you define your own purpose and direction?\n","\n","[Groq Model 2] Concise Summary:\n","Error with Groq Model 2: litellm.NotFoundError: GroqException - {\"error\":{\"message\":\"The model `gemma2-model` does not exist or you do not have access to it.\",\"type\":\"invalid_request_error\",\"code\":\"model_not_found\"}}\n","\n","\n","[Groq Model 3] Detailed Explanation:\n","The error message `Error with Groq Model 2: litellm.NotFoundError: GroqException - {\"error\":{\"message\":\"The model `gemma2-model` does not exist or you do not have access to it.\",\"type\":\"invalid_request_error\",\"code\":\"model_not_found\"}}` indicates that the client is trying to access a non-existent model in the `gemma2-model` Groq schema, which is causing a `NotFound` response.\n","\n","Let's break down the error and the client's response:\n","\n","**Client's Response**\n","\n","The client is sending a request to a Groq API to retrieve the latest version of a model. However, the client is not sure which model it should retrieve, which is causing the `NotFound` response. The client requires validation against the Groq API for specific requests, including:\n","\n","1. `model_not_found`: This is a catch-all error that is thrown when the client does not find the requested model entity.\n","2. `model_not_allowed`: This is an error that is thrown when the client tries to access a non-existent model or a model with the wrong mapping.\n","\n","**Groq API Response**\n","\n","The Groq API will respond with a `model_not_found` error message (`{\"error\":{\"message\":\"The model `gemma2-model` does not exist or you do not have access to it.\",\"type\":\"invalid_request_error\",\"code\":\"model_not_found\"}}`) indicating that the requested model (`gemma2-model`) does not exist or the user does not have access to it. The error code (`code=\"model_not_found\"` indicates the specific cause of the error.\n","\n","**Groq Model Insights**\n","\n","To address the client's response, here are the key takeaways:\n","\n","1. **Verify the API documentation**: The client should verify the API documentation to understand the allowed fields and mapping for retrieving a model.\n","2. **Implement model validation**: The client should implement model validation to ensure that the requested model exists and has the correct mapping before making the request.\n","3. **Use the `all_fields` option**: When retrieving a model, the client can use the `all_fields` option to get all the fields that are allowed to be missed. This can help to reduce the `model_not_found` error and improve performance.\n","\n","Here's an example of how the client can modify their request to incorporate these insights:\n","\n","```groq\n","// Retrieve the most recent version of the gemma model\n","model: { id: 123, foo: 42 } | $allFields\n","\n","// Specify the field that doesn't exist\n","model: { id: 123, non_existent: string } | $allFields\n","```\n","\n","By implementing model validation and using the `all_fields` option, the client can minimize the `model_not_found` error and ensure that the requested model is successfully retrieved.\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/AmmarAamir786/Run_Ollama_In_Colab/blob/main/Run_Ollama_In_Colab.ipynb","timestamp":1739338460889}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}