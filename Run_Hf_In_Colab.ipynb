{"cells":[{"cell_type":"markdown","source":["#**Using Transformers Library by Hugging Face**\n","---\n","\n","Hugging Face offers a wide variety of models for different NLP tasks, powered by its state-of-the-art **Transformers** library.\n","\n","This library provides pre-trained models for tasks such as text classification, question answering, and text generation, enabling users to leverage advanced NLP capabilities without the need for extensive training from scratch.\n","\n","For quick prototyping, the **pipeline** abstraction offers a simple and efficient solution.\n","\n","However, for greater control and customization, using **AutoTokenizer and AutoModelForCausalLM** allows developers to fine-tune models to better suit their specific needs."],"metadata":{"id":"V4K0oIAJxo2I"}},{"cell_type":"code","source":["# Install the Hugging Face Transformers library\n","# This library provides state-of-the-art pre-trained models for NLP tasks\n","!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Pwpcd5EjuyZ","executionInfo":{"status":"ok","timestamp":1739728488360,"user_tz":-300,"elapsed":3039,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}},"outputId":"42ad73e6-1e1d-4591-c91f-954d3e696135"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"]}]},{"cell_type":"markdown","source":["##**Example 1: Sentiment Analysis**\n","This example uses a pre-trained sentiment analysis model to classify the input text as either positive or negative. The pipeline abstraction simplifies the process of loading models and performing inference."],"metadata":{"id":"yFIWnCY5j2Bn"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Initialize the sentiment-analysis pipeline\n","# This pipeline uses a pre-trained model to analyze the sentiment of input text\n","# It classifies the text as either 'POSITIVE' or 'NEGATIVE' with a confidence score\n","sentiment_analyzer = pipeline('sentiment-analysis')\n","\n","# Analyze sentiment of a sample text\n","# The model processes the input and predicts the sentiment\n","result = sentiment_analyzer(\"I love using the Transformers library!\")\n","\n","# Display the result, showing the label and the confidence score\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NvaAMKAEj06A","executionInfo":{"status":"ok","timestamp":1739728515057,"user_tz":-300,"elapsed":26704,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}},"outputId":"6edd03e4-fc4b-4e44-f49b-ec09dfd6e4a9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["[{'label': 'POSITIVE', 'score': 0.9993904829025269}]\n"]}]},{"cell_type":"markdown","source":["##**Example 2: Question Answering**\n","\n","This demonstrates how to use the question-answering pipeline to extract relevant answers from a given context. It showcases Hugging Face's ability to perform information retrieval tasks."],"metadata":{"id":"NdWr-G0cj87K"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Initialize the question-answering pipeline\n","# This pipeline uses a model fine-tuned for extracting answers from a given context\n","qa_pipeline = pipeline('question-answering')\n","\n","# Define the context and the question\n","# The context provides the necessary background information from which the model extracts the answer\n","context = \"\"\"\n","Hugging Face is a technology company based in New York and Paris.\n","It is known for creating tools for natural language processing (NLP).\n","Their Transformers library has become a popular open-source framework for building state-of-the-art NLP models.\n","\"\"\"\n","question = \"Where is Hugging Face based?\"\n","\n","# Get the answer from the model\n","# The model searches the context to find the most relevant answer to the question\n","result = qa_pipeline(question=question, context=context)\n","\n","# Display the answer along with the confidence score\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hvqeMBTtj_Ku","executionInfo":{"status":"ok","timestamp":1739728516794,"user_tz":-300,"elapsed":1750,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}},"outputId":"4595197e-8a24-4ec0-a795-2b4187fbc2c1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n","Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["{'score': 0.9742932915687561, 'start': 47, 'end': 65, 'answer': 'New York and Paris'}\n"]}]},{"cell_type":"markdown","source":["##**Example 3: Text Generation and Conversational AI**\n","\n","**Explanation**: This example illustrates text generation and conversational AI using different models:\n","\n","1.   **facebook/opt-1.3b**: Shows the continuation-style text generation.\n","2.   **facebook/blenderbot-400M-distill**: Demonstrates more conversational and context-aware responses."],"metadata":{"id":"eOh19_-HkFDe"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Initialize the text-generation pipeline with a specific model\n","# Using the 'facebook/opt-1.3b' model, which is optimized for generating coherent text continuations\n","text_generator = pipeline('text-generation', model='facebook/opt-1.3b')\n","\n","# Generate text based on a prompt\n","# The model continues the input text, generating a suitable review for a restaurant\n","result = text_generator(\"Suitable review for restaurant\")\n","print(result[0]['generated_text'])\n","\n","# Generate text based on another prompt\n","# Demonstrates the model's limitation in answering questions, as it tends to repeat the input\n","result = text_generator(\"What does the Hugging Face Transformers library provide?\")\n","print(\"Result = \", result)\n","print(result[0]['generated_text'])\n","\n","# Using a conversational model for better dialogue generation\n","from transformers import pipeline\n","\n","# Initialize a conversational AI model\n","# The 'facebook/blenderbot-400M-distill' model is fine-tuned for generating more interactive and relevant responses\n","chatbot = pipeline('text-generation', model='facebook/blenderbot-400M-distill')\n","\n","# Generate a conversational response based on a prompt\n","# Using a higher temperature to introduce more creativity in the generated response\n","result = chatbot(\"What do you know about the statue of liberty?\", temperature=1.0)\n","print(result[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wlh-FvqzkizO","executionInfo":{"status":"ok","timestamp":1739728537091,"user_tz":-300,"elapsed":20304,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}},"outputId":"01d6ddbd-5711-4fdd-bec5-fe15d36186ec"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["Suitable review for restaurant.\n","\n","I was very impressed with the food and service at this\n","Result =  [{'generated_text': 'What does the Hugging Face Transformers library provide?\\n\\nThe Hugging Face Transformers library provides'}]\n","What does the Hugging Face Transformers library provide?\n","\n","The Hugging Face Transformers library provides\n"]},{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["What do you know about the statue of liberty?    \"Decorate\"\n"]}]},{"cell_type":"markdown","source":["**Notes**:\n","\n","1.   The facebook/opt-1.3b model is not ideal for question-answering, as it continues the prompt rather than answering the question.\n","2.   The facebook/blenderbot-400M-distill model tends to generate shorter responses due to its smaller size and limited context understanding.\n","3.   To get more detailed paragraph-length answers, consider switching to a more powerful model like facebook/blenderbot-3B or facebook/blenderbot-2.7B, which are designed for richer and more informative conversations. Alternatively, you can try facebook/opt-2.7b or gpt2-xl models, which generally provide more detailed and context-aware outputs.\n"],"metadata":{"id":"XrQdU4C_18jb"}},{"cell_type":"code","source":["chatbot = pipeline('text-generation', model='gpt2')\n","result = chatbot(\"What do you know about the statue of liberty?\", temperature=0.7)\n","print(result[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xhQqoSV2030Q","executionInfo":{"status":"ok","timestamp":1739728538462,"user_tz":-300,"elapsed":1376,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}},"outputId":"153d8d2f-290d-47c3-c07f-ded9b5cf2c21"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["What do you know about the statue of liberty? Well, the statue of liberty is not only a statue of liberty but also a sacred thing. It is also sacred to all men.\"\n","\n","The statue of liberty is a symbol of a people who\n"]}]},{"cell_type":"markdown","source":["##**Example 4: Complex Q&A App with State Management**\n","**Explanation**: This complex example illustrates:\n","\n","1.   Direct usage of AutoTokenizer and AutoModelForCausalLM.\n","2.   State management by maintaining conversation history.\n","3.   Usage of GPT-2 for generating context-aware responses.\n","4.   The challenge of maintaining meaningful conversation flow due to the model's lack of fine-tuning for dialogue.\n","\n","**Key Takeaways**:\n","\n","1.   GPT-2 is a general-purpose model and not optimized for Q&A or dialogue, which might result in inconsistent responses.\n","2.   This demonstrates the complexity involved in building conversational applications using Hugging Face Transformers."],"metadata":{"id":"1Mg5kpnzlhuE"}},{"cell_type":"code","source":["## Hugging Face Transformers LLM Integration - Q&A App Demo\n","\n","# Install necessary libraries\n","# Torch is required for model computations\n","!pip install transformers torch\n","\n","# Import necessary modules from Hugging Face Transformers and PyTorch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","# Load pre-trained model and tokenizer\n","# This example uses GPT-2, a general-purpose language model\n","# Loading the model and tokenizer can take some time, especially without GPU acceleration\n","model_name = 'gpt2'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# Function to generate responses, maintaining conversation history\n","# This function demonstrates state management by keeping track of user inputs and bot responses\n","conversation_history = []\n","def ask_question(user_input):\n","    # Store the user's input\n","    conversation_history.append(f'User: {user_input}')\n","\n","    # Tokenize the conversation history for input to the model\n","    inputs = tokenizer(' '.join(conversation_history), return_tensors='pt')\n","\n","    # Generate a response using the model\n","    # torch.no_grad() is used to disable gradient calculations, improving performance\n","    with torch.no_grad():\n","        outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id)\n","\n","    # Decode the generated response and clean up the output\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True).split('User:')[-1].strip()\n","    conversation_history.append(f'Bot: {response}')\n","\n","    # Return the generated response\n","    return response\n","\n","# Example interaction demonstrating state management and continuity\n","user_input = \"What is Hugging Face?\"\n","print(\"User:\", user_input)\n","response = ask_question(user_input)\n","print(\"Bot:\", response)\n","\n","user_input = \"What is transformers library used for?\"\n","print(\"\\nUser:\", user_input)\n","response = ask_question(user_input)\n","print(\"Bot:\", response)\n","\n","# Displaying state management complexity\n","# Showing how the model retains context and conversation history\n","print(\"\\nConversation History:\")\n","for line in conversation_history:\n","    print(line)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKpl7e0plpmV","executionInfo":{"status":"ok","timestamp":1739728544458,"user_tz":-300,"elapsed":6001,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}},"outputId":"ac9baea6-e785-4ea7-fb44-3b40ec0451a0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","User: What is Hugging Face?\n","Bot: What is Hugging Face?\n","\n","User: What is transformers library used for?\n","Bot: What is transformers library used for?\n","\n","Conversation History:\n","User: What is Hugging Face?\n","Bot: What is Hugging Face?\n","User: What is transformers library used for?\n","Bot: What is transformers library used for?\n"]}]},{"cell_type":"code","source":["#For this part of the example, we'll use the microsoft/DialoGPT-medium model, a fine-tuned version of GPT-2 for conversational purposes.\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","# Load the pre-trained model and tokenizer\n","model_name = \"microsoft/DialoGPT-medium\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# Initialize the chat history\n","chat_history_ids = None\n","\n","print(\"Chatbot: Hello! I'm an open-source chatbot. Type 'exit' to end the conversation.\")\n","\n","while True:\n","    # Get user input\n","    user_input = input(\"User: \")\n","    if user_input.lower() == \"exit\":\n","        print(\"Chatbot: Goodbye!\")\n","        break\n","\n","    # Encode the user input and add end-of-string token\n","    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n","\n","    # Append the new user input to the chat history\n","    if chat_history_ids is not None:\n","        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n","    else:\n","        bot_input_ids = new_user_input_ids\n","\n","    # Generate a response\n","    chat_history_ids = model.generate(\n","        bot_input_ids,\n","        max_length=1000,\n","        pad_token_id=tokenizer.eos_token_id,\n","        no_repeat_ngram_size=3,\n","        do_sample=True,\n","        top_k=50,\n","        top_p=0.95,\n","        temperature=0.75\n","    )\n","\n","    # Decode the response\n","    bot_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n","\n","    print(f\"Chatbot: {bot_response}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-xzA8yc4csK","executionInfo":{"status":"ok","timestamp":1739728621569,"user_tz":-300,"elapsed":77114,"user":{"displayName":"Bilal KSK","userId":"06984443272301995348"}},"outputId":"92621895-96e5-4729-f3f8-34c3024c33df"},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":["Chatbot: Hello! I'm an open-source chatbot. Type 'exit' to end the conversation.\n","User: hi. what is color of apples?\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"output_type":"stream","name":"stdout","text":["Chatbot: Sugar, butter, lemon, and water.\n","User: i am asking color of apples?\n","Chatbot: Blue, purple and yellow.\n","User: just one.\n","Chatbot: Yes, just one.\n","User: exit\n","Chatbot: Goodbye!\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/AmmarAamir786/Run_Ollama_In_Colab/blob/main/Run_Ollama_In_Colab.ipynb","timestamp":1739338460889}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}